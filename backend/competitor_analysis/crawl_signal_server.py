import pymysql
import time
from datetime import datetime
from backend.competitor_analysis.crawler import crawl_reviews_by_category
from backend.competitor_analysis.differentiator import summarize_competitor_reviews
from backend.competitor_analysis.competitor_db import insert_review_summary
from utils.logger import get_logger

logger = get_logger(__name__)

def send_crawl_request_signal(host, user, password, db, category):
    """
    í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹ í˜¸ë¥¼ crawl_request_signal í…Œì´ë¸”ì— ë“±ë¡(ìš”ì²­)í•©ë‹ˆë‹¤.

    Args:
        host (str): DB í˜¸ìŠ¤íŠ¸ ì£¼ì†Œ
        user (str): DB ì‚¬ìš©ìëª…
        password (str): DB ë¹„ë°€ë²ˆí˜¸
        db (str): ë°ì´í„°ë² ì´ìŠ¤ëª…
        category (str): í¬ë¡¤ë§ì„ ìš”ì²­í•  ìƒí’ˆ ì¹´í…Œê³ ë¦¬

    Returns:
        None
    """
    logger.debug(f"ğŸ› ï¸ í¬ë¡¤ëŸ¬ ìš”ì²­ ì‹ í˜¸ ì „ì†¡ ì‹œì‘: {category}")
    try:
        conn = pymysql.connect(
            host=host, user=user, password=password, db=db, port=3306, charset='utf8mb4'
        )
        with conn.cursor() as cur:
            sql = """
            INSERT INTO crawl_request_signal (category, status, requested_at)
            VALUES (%s, 'pending', %s)
            """
            cur.execute(sql, (category, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
        conn.commit()
        logger.info(f"âœ… í¬ë¡¤ëŸ¬ ìš”ì²­ ì‹ í˜¸ ì „ì†¡ ì™„ë£Œ: {category}")
    except Exception as e:
        logger.error(f"âŒ ì‹ í˜¸ ì „ì†¡ ì‹¤íŒ¨: {type(e).__name__}: {e!r}")
    finally:
        try:
            conn.close()
            logger.debug("ğŸ› ï¸ DB ì—°ê²° ì¢…ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ DB ì—°ê²° ì¢…ë£Œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e!r}")

def poll_and_process_signal(host, user, password, db, openai_api_key, interval=10):
    """
    ì‹ í˜¸(pending) ê°ì§€ ì‹œ â†’ 1. ì¹´í…Œê³ ë¦¬ ë¦¬ë·° í¬ë¡¤ë§ â†’ 2. ìš”ì•½ â†’ 3. DB ì €ì¥ê¹Œì§€ ìë™ ìˆ˜í–‰.

    Args:
        host (str): DB í˜¸ìŠ¤íŠ¸ ì£¼ì†Œ
        user (str): DB ì‚¬ìš©ìëª…
        password (str): DB ë¹„ë°€ë²ˆí˜¸
        db (str): ë°ì´í„°ë² ì´ìŠ¤ëª…
        openai_api_key (str): OpenAI API í‚¤
        interval (int, optional): polling ì£¼ê¸°(ì´ˆ), ê¸°ë³¸ê°’ 10ì´ˆ

    Returns:
        None
    """
    logger.debug(f"ğŸ› ï¸ ì‹ í˜¸ polling ë£¨í”„ ì‹œì‘ (interval={interval}s)")
    while True:
        try:
            conn = pymysql.connect(
                host=host, user=user, password=password, db=db, port=3306, charset='utf8mb4'
            )
            with conn.cursor() as cur:
                # ê°€ì¥ ì˜¤ë˜ëœ ë¯¸ì²˜ë¦¬ ì‹ í˜¸(ì¹´í…Œê³ ë¦¬) ì¡°íšŒ
                cur.execute("""
                    SELECT id, category FROM crawl_request_signal
                    WHERE status='pending' ORDER BY requested_at ASC LIMIT 1
                """)
                row = cur.fetchone()
                if row:
                    signal_id, category = row
                    logger.info(f"âœ… ì‹ í˜¸ ê°ì§€: '{category}' í¬ë¡¤ë§ ë° ìš”ì•½ ì‹œì‘")

                    # 1ï¸ê²½ìŸì‚¬ ë¦¬ë·° í¬ë¡¤ë§
                    reviews = crawl_reviews_by_category(
                        category=category,
                        max_products=3,
                        max_reviews_per_product=10
                    )
                    logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(reviews)}ê°œ ë¦¬ë·°")

                    if not reviews:
                        logger.warning(f"âš ï¸ ìˆ˜ì§‘ëœ ë¦¬ë·° ì—†ìŒ, ì‹ í˜¸ë§Œ ì™„ë£Œ ì²˜ë¦¬")
                    else:
                        # 2ï¸ë¦¬ë·° ìš”ì•½
                        summary = summarize_competitor_reviews(reviews, openai_api_key)
                        logger.info("âœ… ë¦¬ë·° ìš”ì•½ ì™„ë£Œ")

                        # 3ï¸DB ì €ì¥
                        insert_review_summary(
                            host=host,
                            user=user,
                            password=password,
                            db=db,
                            category=category,
                            review_summary=summary,
                            num_reviews=len(reviews)
                        )
                        logger.info("âœ… ë¦¬ë·° ìš”ì•½ë³¸ DB ì €ì¥ ì™„ë£Œ")

                    # ì‹ í˜¸ ì™„ë£Œ ì²˜ë¦¬ (done í‘œì‹œ ë° ì™„ë£Œ ì‹œê°)
                    cur.execute("""
                        UPDATE crawl_request_signal
                        SET status='done', completed_at=%s
                        WHERE id=%s
                    """, (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), signal_id))
                    conn.commit()
                    logger.info(f"âœ… ì‹ í˜¸ ì²˜ë¦¬ ì™„ë£Œ: '{category}'")
                else:
                    logger.debug("ğŸ› ï¸ ì²˜ë¦¬ ëŒ€ê¸° ì‹ í˜¸ ì—†ìŒ (ë‹¤ìŒ í™•ì¸ê¹Œì§€ ëŒ€ê¸°)")
            conn.close()
        except Exception as e:
            logger.error(f"âŒ ì‹ í˜¸ ê°ì§€/ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {type(e).__name__}: {e!r}")
        time.sleep(interval)