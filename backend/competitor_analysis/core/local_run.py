import threading
import time
from backend.competitor_analysis.core.crawler import crawl_reviews_by_category
from backend.competitor_analysis.core.differentiator import summarize_competitor_reviews
from backend.competitor_analysis.core.competitor_db import insert_review_summary
from backend.competitor_analysis.core.crawl_signal_server import poll_and_process_signal
from utils.logger import get_logger

logger = get_logger(__name__)

def auto_crawl_loop(
    db_config,
    openai_api_key,
    category_list,
    interval=1800  # 30ë¶„ë§ˆë‹¤ (ì´ˆ ë‹¨ìœ„)
):
    """
    ì—¬ëŸ¬ ìœ ëª… ì¹´í…Œê³ ë¦¬ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì£¼ê¸°ì ìœ¼ë¡œ í¬ë¡¤ë§/ìš”ì•½/DB ì €ì¥ì„ ìë™ ìˆ˜í–‰í•˜ëŠ” ë£¨í”„.

    Args:
        db_config (dict): DB ì—°ê²°ì •ë³´ (host, user, password, db)
        openai_api_key (str): OpenAI API í‚¤
        category_list (list): í¬ë¡¤ë§ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸
        interval (int): ë°˜ë³µ ì£¼ê¸°(ì´ˆ) - ê¸°ë³¸ 1800ì´ˆ(30ë¶„)
    """
    logger.info(f"âœ… ìë™ í¬ë¡¤ë§ ë£¨í”„ ì‹œì‘ (ì£¼ê¸°: {interval}s)")
    while True:
        for category in category_list:
            try:
                logger.info(f"âœ… ìë™ í¬ë¡¤ë§ ì‹œë„: {category}")
                reviews = crawl_reviews_by_category(
                    category,
                    max_products=3,
                    max_reviews_per_product=10
                )
                if not reviews:
                    logger.warning(f"âš ï¸ ë¦¬ë·° ì—†ìŒ: {category}")
                    continue
                summary = summarize_competitor_reviews(reviews, openai_api_key)
                insert_review_summary(
                    host=db_config["host"],
                    user=db_config["user"],
                    password=db_config["password"],
                    db=db_config["db"],
                    category=category,
                    review_summary=summary,
                    num_reviews=len(reviews)
                )
                logger.info(f"âœ… ìë™ í¬ë¡¤ë§+ìš”ì•½+DBì €ì¥ ì™„ë£Œ: {category}")
            except Exception as e:
                logger.error(f"âŒ ìë™ í¬ë¡¤ë§/ìš”ì•½/ì €ì¥ ì‹¤íŒ¨: {type(e).__name__}: {e!r}")
        logger.info(f"ğŸ› ï¸ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì™„ë£Œ, {interval}ì´ˆ ëŒ€ê¸°")
        time.sleep(interval)

def start_local_crawler(
    db_config,
    openai_api_key,
    category_list,
    auto_crawl_interval=1800,
    signal_poll_interval=10
):
    """
    ë¡œì»¬ í¬ë¡¤ëŸ¬ë¥¼ ì‹œì‘:
    - 1) ìœ ëª… ì¹´í…Œê³ ë¦¬ ìë™ í¬ë¡¤ë§/ìš”ì•½/ì €ì¥ ë£¨í”„
    - 2) GCP ë“±ì—ì„œ ì‹ í˜¸ê°€ ì˜¤ë©´ polling í›„ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ì¦‰ì‹œ í¬ë¡¤ë§/ì €ì¥
    ë¥¼ ë™ì‹œì— ë³‘ë ¬(ìŠ¤ë ˆë“œ)ë¡œ ì‹¤í–‰

    Args:
        db_config (dict): DB ì—°ê²°ì •ë³´ (host, user, password, db)
        openai_api_key (str): OpenAI API í‚¤
        category_list (list): í¬ë¡¤ë§ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸
        auto_crawl_interval (int): ìë™ í¬ë¡¤ë§ ë£¨í”„ ì£¼ê¸°(ì´ˆ)
        signal_poll_interval (int): ì‹ í˜¸ polling ì£¼ê¸°(ì´ˆ)
    """
    # 1. ìë™ í¬ë¡¤ë§ìš© ìŠ¤ë ˆë“œ
    t1 = threading.Thread(
        target=auto_crawl_loop,
        args=(db_config, openai_api_key, category_list, auto_crawl_interval),
        daemon=True
    )
    t1.start()

    # 2. ì‹ í˜¸ polling/ì²˜ë¦¬ìš© ìŠ¤ë ˆë“œ (ì‹ í˜¸ ê°ì§€ ì‹œ â†’ ì¹´í…Œê³ ë¦¬ ë¦¬ë·° í¬ë¡¤ë§/ìš”ì•½/ì €ì¥)
    t2 = threading.Thread(
        target=poll_and_process_signal,
        args=(
            db_config["host"],
            db_config["user"],
            db_config["password"],
            db_config["db"],
            openai_api_key,
            signal_poll_interval
        ),
        daemon=True
    )
    t2.start()

    logger.info("âœ… ìë™ í¬ë¡¤ë§/ì‹ í˜¸ polling ìŠ¤ë ˆë“œ ëª¨ë‘ ì‹œì‘")
    # ë©”ì¸ ìŠ¤ë ˆë“œëŠ” ë¬´í•œ ëŒ€ê¸°
    while True:
        time.sleep(3600)


if __name__ == "__main__":
    from utils.config import get_openai_api_key, get_db_config

    logger.info("main ì§„ì… í™•ì¸")  # logë„ ë‚¨ê¹€

    # 1. ì„¤ì •/í‚¤ ë¶ˆëŸ¬ì˜¤ê¸° (utils í•¨ìˆ˜ ê·¸ëŒ€ë¡œ í™œìš©)
    openai_api_key = get_openai_api_key()
    db_config = get_db_config()

    # 2. í…ŒìŠ¤íŠ¸ìš© ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ (or ì‹¤ì œ ìš´ì˜ ì¹´í…Œê³ ë¦¬)
    category_list = [
        # íŒ¨ì…˜/ì˜ë¥˜
        "ë¸”ë¼ìš°ìŠ¤", "ë‚¨ì„± ë°˜íŒ”í‹°", "ì—¬ì„± ë°˜íŒ”í‹°", "ì²­ë°”ì§€", "ì›í”¼ìŠ¤", "ë‚¨ì„± ì…”ì¸ ", "ì—¬ì„± ê°€ë””ê±´",
        "ì•„ë™ë³µ", "ìš´ë™ë³µ", "ë ˆê¹…ìŠ¤",

        # íŒ¨ì…˜ ì¡í™”/ì‹ ë°œ/ê°€ë°©
        "ìš´ë™í™”", "ìƒŒë“¤", "ìŠ¬ë¦¬í¼", "í† íŠ¸ë°±", "ì—ì½”ë°±", "ì§€ê°‘", "ëª¨ì", "ì–‘ë§",

        # ì£¼ë°©/ìƒí™œìš©í’ˆ
        "ì£¼ë°©ìš©í’ˆ", "í”„ë¼ì´íŒ¬", "ëƒ„ë¹„", "ìˆ˜ì €ì„¸íŠ¸", "ë„ë§ˆ", "ë°€íìš©ê¸°", "ì£¼ì „ì",
        "í™”ì¥ì§€", "ì£¼ë°©ì„¸ì œ", "ìš•ì‹¤ìš©í’ˆ", "ì²­ì†Œë„êµ¬",

        # ê°€ì „/ë””ì§€í„¸
        "ë¬´ì„  ì´ì–´í°", "íœ´ëŒ€í°", "íœ´ëŒ€í° ì¼€ì´ìŠ¤", "ë…¸íŠ¸ë¶", "ë§ˆìš°ìŠ¤", "í‚¤ë³´ë“œ", "ëª¨ë‹ˆí„°",
        "ìŠ¤ë§ˆíŠ¸ì›Œì¹˜", "ë¸”ë£¨íˆ¬ìŠ¤ ìŠ¤í”¼ì»¤", "ê³µê¸°ì²­ì •ê¸°", "ì „ê¸°í¬íŠ¸", "ì»¤í”¼ë¨¸ì‹ ",

        # ë·°í‹°/ë¯¸ìš©
        "ìŠ¤í‚¨ì¼€ì–´", "ë¡œì…˜", "í´ë Œì§•í¼", "ì„ í¬ë¦¼", "ë¦½ìŠ¤í‹±", "ë§ˆìŠ¤ì¹´ë¼", "í—¤ì–´ë“œë¼ì´ê¸°", "ê³ ë°ê¸°",

        # ì‹í’ˆ/ê±´ê°•
        "ì¦‰ì„ë°¥", "ìƒìˆ˜", "ë¼ë©´", "ì»¤í”¼ë¯¹ìŠ¤", "ë¹„íƒ€ë¯¼", "ìœ ì‚°ê· ", "ë‹­ê°€ìŠ´ì‚´",

        # ìœ ì•„/ì™„êµ¬/ë°˜ë ¤ë™ë¬¼
        "ê¸°ì €ê·€", "ë¬¼í‹°ìŠˆ", "ì•„ê¸° ì¥ë‚œê°", "ì• ê²¬ì‚¬ë£Œ", "ê³ ì–‘ì´ëª¨ë˜", "ë°˜ë ¤ë™ë¬¼ ìš©í’ˆ",

        # ì·¨ë¯¸/ë¬¸êµ¬/ìŠ¤í¬ì¸ 
        "ë³¼íœ", "ë…¸íŠ¸", "ë‹¤ì´ì–´ë¦¬", "í¼ì¦", "ìš´ë™í™”", "ìš”ê°€ë§¤íŠ¸", "ìì „ê±° ìš©í’ˆ",

        # ê³„ì ˆ/ê¸°íƒ€
        "ì„ í’ê¸°", "ì˜¨í’ê¸°", "ê°€ìŠµê¸°", "ìš°ì‚°", "í…€ë¸”ëŸ¬", "ë§ˆìŠ¤í¬"
    ]

    # 3. í¬ë¡¤ëŸ¬ ì‹œì‘
    try:
        start_local_crawler(
            db_config=db_config,
            openai_api_key=openai_api_key,
            category_list=category_list,
            auto_crawl_interval=36000,   # 10ì‹œê°„ ë§ˆë‹¤ ìë™ ì €ì¥
            signal_poll_interval=5     # 5ì´ˆë§ˆë‹¤ polling
        )
    except Exception as e:
        logger.error(f"âŒ main ì§„ì…/ì‹¤í–‰ ì‹¤íŒ¨: {type(e).__name__}: {e!r}")
        print("main ì˜ˆì™¸:", e)