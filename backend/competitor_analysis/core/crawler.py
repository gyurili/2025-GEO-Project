from typing import List
import time, re
from urllib.parse import quote
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from utils.logger import get_logger

logger = get_logger(__name__)


# =========================
# ë“œë¼ì´ë²„ ì´ˆê¸°í™”
# =========================
def init_safe_driver():
    options = uc.ChromeOptions()
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                         "(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
    options.add_argument("--window-size=1920,1080")
    driver = uc.Chrome(options=options)

    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": """
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3]});
            Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']});
            window.chrome = { runtime: {} };
        """
    })

    return driver


# =========================
# ë¦¬ë·° ìˆ˜ì§‘ (2ì  ì´í•˜ í•„í„°ë§)
# =========================
def crawl_reviews_by_link(driver, url: str, max_reviews: int = 30) -> List[str]:
    logger.debug(f"ğŸ› ï¸ ìƒí’ˆ í˜ì´ì§€ ì ‘ì† ì‹œë„: {url}")
    reviews = []

    try:
        driver.get(url)
        time.sleep(3)

        # ë¦¬ë·° íƒ­ í´ë¦­
        try:
            review_tab = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "li.sdp-review__article-tab-item[data-tab-name='REVIEW']"))
            )
            review_tab.click()
            logger.info("âœ… ë¦¬ë·° íƒ­ í´ë¦­ ì™„ë£Œ")
            time.sleep(2)
        except Exception as e:
            logger.error(f"âŒ ë¦¬ë·° íƒ­ í´ë¦­ ì‹¤íŒ¨: {e}")
            return []

        # ë³„ì  ë‚®ì€ ìˆœ ì •ë ¬
        try:
            sort_btn = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "button.sdp-review__article__order__selected"))
            )
            sort_btn.click()
            time.sleep(1)

            low_rating_option = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "ul.sdp-review__article__order__list li[data-option-value='scoreAsc']"))
            )
            low_rating_option.click()
            logger.info("âœ… ë³„ì  ë‚®ì€ ìˆœ ì •ë ¬ ì™„ë£Œ")
            time.sleep(2)
        except Exception as e:
            logger.warning(f"âš ï¸ ë³„ì  ì •ë ¬ ì˜µì…˜ í´ë¦­ ì‹¤íŒ¨ (ê¸°ë³¸ ì •ë ¬ ìœ ì§€): {e}")

        # ìŠ¤í¬ë¡¤ & ë”ë³´ê¸° ë°˜ë³µ
        for _ in range(10):
            driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
            time.sleep(1.5)
            try:
                more_btn = driver.find_element(By.CSS_SELECTOR, "button.sdp-review__article__order__option__more")
                more_btn.click()
                time.sleep(1.5)
            except:
                break

        # ë¦¬ë·° íŒŒì‹± + 2ì  ì´í•˜ë§Œ í•„í„°ë§
        soup = BeautifulSoup(driver.page_source, "html.parser")
        review_elements = soup.select("article.sdp-review__article__list__review")
        logger.debug(f"ğŸ› ï¸ ì „ì²´ ë¦¬ë·° ì—˜ë¦¬ë¨¼íŠ¸ ìˆ˜: {len(review_elements)}")

        for el in review_elements:
            rating_tag = el.select_one("span.rating-star")
            rating = None
            if rating_tag:
                aria = rating_tag.get("aria-label", "")
                match = re.search(r"ë³„ì \s*(\d)", aria)
                if match:
                    rating = int(match.group(1))

            if rating is not None and rating > 2:
                continue  # 2ì  ì´í•˜ë§Œ

            text = el.get_text(strip=True)
            text = re.sub(r"[^\w\sê°€-í£.,!?]", "", text)
            if text:
                reviews.append(text)

            if len(reviews) >= max_reviews:
                break

        logger.info(f"âœ… ë³„ì  2ì  ì´í•˜ ë¦¬ë·° {len(reviews)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return reviews

    except Exception as e:
        logger.error(f"âŒ ë¦¬ë·° í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []


# =========================
# ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ â†’ ë³„ì  ë‚®ì€ ì œí’ˆ ë¦¬ë·° ìˆ˜ì§‘
# =========================
def crawl_reviews_by_category(category: str, exclude_name: str = "", max_products: int = 3, max_reviews_per_product: int = 10) -> List[str]:
    logger.debug(f"ğŸ› ï¸ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê²€ìƒ‰ ì‹œì‘: {category}")
    driver = init_safe_driver()
    all_reviews = []

    try:
        base_url = f"https://www.coupang.com/np/search?q={quote(category)}&page=1"
        driver.get(base_url)
        time.sleep(4)

        soup = BeautifulSoup(driver.page_source, "html.parser")
        items = soup.select("li.search-product")
        logger.debug(f"ğŸ› ï¸ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(items)}")

        product_infos = []
        for item in items:
            if item.select_one(".ad-badge-text"):
                continue  # ê´‘ê³  ì œì™¸

            title_tag = item.select_one("div.name")
            link_tag = item.select_one("a.search-product-link")
            rating_tag = item.select_one("em.rating")
            count_tag = item.select_one("span.rating-total-count")

            if not title_tag or not link_tag or not rating_tag or not count_tag:
                continue

            title = title_tag.get_text(strip=True)
            if exclude_name and exclude_name in title:
                continue

            try:
                rating = float(rating_tag.get_text(strip=True))
                rating_count = int(re.sub(r"[^\d]", "", count_tag.get_text()))
            except:
                continue

            link = "https://www.coupang.com" + link_tag["href"]
            product_infos.append((title, link, rating, rating_count))

        product_infos.sort(key=lambda x: x[2])  # ë³„ì  ë‚®ì€ ìˆœ
        selected = product_infos[:max_products]
        logger.info(f"âœ… ë³„ì  ë‚®ì€ ì œí’ˆ {len(selected)}ê°œ ì„ íƒë¨")

        for title, link, rating, count in selected:
            logger.info(f"âœ… [{title}] ë¦¬ë·° ìˆ˜ì§‘ ì¤‘... (í‰ê·  {rating}ì , ë¦¬ë·° ìˆ˜ {count})")
            reviews = crawl_reviews_by_link(driver, link, max_reviews=max_reviews_per_product)
            all_reviews.extend(reviews)
            time.sleep(2)

        logger.info(f"âœ… ì´ ë¦¬ë·° ìˆ˜ì§‘ ê°œìˆ˜: {len(all_reviews)}")
        return all_reviews

    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

    finally:
        driver.quit()
        logger.debug("ğŸ› ï¸ ë“œë¼ì´ë²„ ì¢…ë£Œ")