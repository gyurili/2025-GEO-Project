from typing import List
import time, re
from urllib.parse import quote
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from utils.logger import get_logger
from fake_useragent import UserAgent

logger = get_logger(__name__)


# =========================
# ì…€ë ˆë‹ˆì›€ ì•ˆì „ ì‹¤í–‰ ë“œë¼ì´ë²„ ì´ˆê¸°í™”
# =========================
def init_safe_driver():
    ua = UserAgent()
    user_agent = ua.chrome

    options = uc.ChromeOptions()
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f"user-agent={user_agent}")
    options.binary_location = "/usr/bin/google-chrome"
    driver = uc.Chrome(options=options)
    return driver


# =========================
# ì¿ íŒ¡ ìƒí’ˆ ë¦¬ë·° ìˆ˜ì§‘ (ë³„ì  2ì  ì´í•˜ë§Œ í•„í„°ë§)
# =========================
def crawl_reviews_by_link(driver, url: str, max_reviews: int = 30) -> List[str]:
    logger.debug(f"ğŸ”€ï¸ ì¿ íŒ¡ ìƒí’ˆ ë¦¬ë·° í˜ì´ì§€ ìš”ì²­: {url}")
    reviews = []

    try:
        # ë¦¬ë·° íƒ­ URLì€ ìƒí’ˆ ë§í¬ ë’¤ì— /reviews ì¶”ê°€
        review_url = url + "/reviews"
        driver.get(review_url)
        time.sleep(3)

        # í˜ì´ì§€ íƒìƒ‰ ë° ë¦¬ë·° ìˆ˜ì§‘
        while len(reviews) < max_reviews:
            soup = BeautifulSoup(driver.page_source, "html.parser")
            review_elements = soup.select("div.sdp-review__article__list__review__content")

            for el in review_elements:
                # ë³„ì  ì¶”ì¶œ (ë³„ì ì€ width í¼ì„¼íŠ¸ë¡œ í‘œí˜„ë¨)
                rating_tag = el.find_previous("div", class_="sdp-review__article__list__info__product-info__star-orange")
                rating = None
                if rating_tag:
                    match = re.search(r"width:\s*(\d+)%", str(rating_tag))
                    if match:
                        percentage = int(match.group(1))
                        rating = round(percentage / 20, 1)

                # 2ì  ì´í•˜ë§Œ ìˆ˜ì§‘
                if rating is not None and rating > 2:
                    continue

                text = el.get_text(strip=True)
                text = re.sub(r"[^\w\sê°€-í£.,!?]", "", text)
                if text:
                    reviews.append(text)

                if len(reviews) >= max_reviews:
                    break

            # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ í´ë¦­ (ì—†ìœ¼ë©´ ì¢…ë£Œ)
            try:
                next_btn = driver.find_element(By.CSS_SELECTOR, "button.sdp-review__article__page__button--next")
                if "disabled" in next_btn.get_attribute("class"):
                    break
                next_btn.click()
                time.sleep(2)
            except:
                break

        logger.info(f"âœ… ë³„ì  2ì  ì´í•˜ ë¦¬ë·° {len(reviews)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return reviews

    except Exception as e:
        logger.error(f"âŒ ë¦¬ë·° í¬ë¡¤ë§ ì˜ˆì™¸ ë°œìƒ: {e}")
        return []


# =========================
# ì¿ íŒ¡ ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ â†’ ìƒí’ˆ ëª©ë¡ì—ì„œ ë¦¬ë·° ìˆ˜ì§‘
# =========================
def crawl_reviews_by_category(category: str, max_products: int = 3, max_reviews_per_product: int = 10) -> List[str]:
    logger.debug(f"ğŸ”€ï¸ ì¿ íŒ¡ ê²€ìƒ‰ ì‹œì‘: {category}")
    all_reviews = []
    driver = init_safe_driver()

    try:
        # ì¿ íŒ¡ ê²€ìƒ‰ URL êµ¬ì„± (ë¦¬ë·°ìˆœ ì •ë ¬)
        base_url = f"https://www.coupang.com/np/search?q={quote(category)}&sorter=scoreDesc"
        driver.get(base_url)

        # ë Œë”ë§ ìœ ë„: ìŠ¤í¬ë¡¤ ë‘ ë²ˆ
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(1)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

        # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ìš”ì†Œ ë¡œë”© ëŒ€ê¸°
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located(
                    (By.CSS_SELECTOR, 'ul#product-list li[class^="ProductUnit_productUnit__"]')
                )
            )
            time.sleep(2)
        except Exception as e:
            logger.error(f"â° ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ë¡œë”© ì‹¤íŒ¨ (Timeout): {e}")
            return []

        # ìƒí’ˆ ë§í¬ ìˆ˜ì§‘
        soup = BeautifulSoup(driver.page_source, "html.parser")
        product_links = []

        product_tags = soup.select('ul#product-list li[class^="ProductUnit_productUnit__"]')
        logger.debug(f"ğŸ” ìƒí’ˆ ì—˜ë¦¬ë¨¼íŠ¸ ê°œìˆ˜: {len(product_tags)}")

        for idx, li in enumerate(product_tags):
            a_tag = li.select_one('a[href^="/vp/products/"]')
            if not a_tag:
                continue

            href = a_tag.get("href")
            full_url = "https://www.coupang.com" + href
            product_links.append(full_url)
            logger.debug(f"âœ… ìƒí’ˆ {idx + 1}: {full_url}")

            if len(product_links) >= max_products:
                break

        logger.info(f"âœ… ë¦¬ë·° ìˆ˜ì§‘ ëŒ€ìƒ ìƒí’ˆ ìˆ˜: {len(product_links)}")

        # ê° ìƒí’ˆë³„ ë¦¬ë·° ìˆ˜ì§‘
        for link in product_links:
            logger.info(f"âœ… ë¦¬ë·° ìˆ˜ì§‘ ì¤‘: {link}")
            reviews = crawl_reviews_by_link(driver, link, max_reviews=max_reviews_per_product)
            all_reviews.extend(reviews)
            time.sleep(2)

        logger.info(f"âœ… ì´ ìˆ˜ì§‘ ë¦¬ë·° ìˆ˜: {len(all_reviews)}")
        return all_reviews

    except Exception as e:
        logger.error(f"âŒ ì¿ íŒ¡ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

    finally:
        driver.quit()
        logger.debug("ğŸ› ï¸ ë“œë¼ì´ë²„ ì¢…ë£Œ")