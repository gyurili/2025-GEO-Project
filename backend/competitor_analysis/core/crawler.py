from typing import List
import time, re
from urllib.parse import quote
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from utils.logger import get_logger

logger = get_logger(__name__)


def init_safe_driver():
    options = uc.ChromeOptions()
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--window-size=1920,1080")
    # options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
    options.binary_location = "/usr/bin/google-chrome"
    driver = uc.Chrome(options=options)
    return driver


# =========================
# ë¦¬ë·° ìˆ˜ì§‘ (2ì  ì´í•˜ í•„í„°ë§)
# =========================
def crawl_reviews_by_link(driver, url: str, max_reviews: int = 30) -> List[str]:
    logger.debug(f"ğŸ”€ï¸ ë„¤ì´ë²„ ìƒí’ˆ ë¦¬ë·° í˜ì´ì§€ ìš”ì²­: {url}")
    reviews = []

    try:
        driver.get(url)
        time.sleep(3)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        review_elements = soup.select(".reviewItems_review_content__0Q2Tg")
        logger.debug(f"ğŸ”€ï¸ ì „ì²´ ë¦¬ë·° ì—˜ë¦¬ë¨¼íŠ¸ ìˆ˜: {len(review_elements)}")

        for el in review_elements:
            rating_tag = el.find_previous("span", class_="reviewItems_average__F6aF2")
            rating = None
            if rating_tag:
                match = re.search(r"[0-9.]+", rating_tag.get_text())
                if match:
                    rating = float(match.group(0))

            if rating is not None and rating > 2:
                continue  # 2ì  ì´í•˜ë§Œ ìˆ˜ì§‘

            text = el.get_text(strip=True)
            text = re.sub(r"[^\w\sê°€-í£.,!?]", "", text)
            if text:
                reviews.append(text)

            if len(reviews) >= max_reviews:
                break

        logger.info(f"âœ… ë³„ì  2ì  ì´í•˜ ë¦¬ë·° {len(reviews)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return reviews

    except Exception as e:
        logger.error(f"âŒ ë¦¬ë·° í¬ë¡¤ë§ ì‹œë„ ì˜ˆì™¸ ë°œìƒ: {e}")
        return []


# =========================
# ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ â†’ ë³„ì  ë‚®ì€ ì œí’ˆ ë¦¬ë·° ìˆ˜ì§‘
# =========================
def crawl_reviews_by_category(category: str, max_products: int = 3, max_reviews_per_product: int = 10) -> List[str]:
    logger.debug(f"ğŸ”€ï¸ ë„¤ì´ë²„ ì‡¼í•‘ ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ì‹œì‘: {category}")
    all_reviews = []
    driver = init_safe_driver()

    try:
        base_url = f"https://search.shopping.naver.com/search/all?query={quote(category)}&sort=review&rating=low"
        driver.get(base_url)
        time.sleep(4)
        
        screenshot_path = f"debug_naver_{category}.png"
        driver.save_screenshot(screenshot_path)
        logger.info(f"ğŸ“¸ ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì™„ë£Œ: {screenshot_path}")
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        items = soup.select("ul.compositeCardList_product_list__hj4JR > li.compositeCardContainer_composite_card_container__r8c8b")
        logger.debug(f"ğŸ”€ï¸ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(items)}")

        product_infos = []
        for item in items:
            link_tag = item.select_one("a.basicProductCard_link__urzND")
            title_tag = item.select_one("strong.basicProductCard_title__VfX3c")
            rating_tag = item.select_one("span.basicProductCard_average__YGapX")

            if not link_tag or not title_tag or not rating_tag:
                continue

            try:
                rating = float(rating_tag.get_text(strip=True))
            except:
                continue

            link = link_tag["href"]
            if link.startswith("/"):
                link = "https://shopping.naver.com" + link

            title = title_tag.get_text(strip=True)
            product_infos.append((title, link, rating))

        product_infos.sort(key=lambda x: x[2])  # ë³„ì  ë‚®ì€ ìˆœ
        selected = product_infos[:max_products]
        logger.info(f"âœ… ë³„ì  ë‚®ì€ ì œí’ˆ {len(selected)}ê°œ ì„ íƒë¨")

        for title, link, rating in selected:
            logger.info(f"âœ… [{title}] ë¦¬ë·° ìˆ˜ì§‘ ì¤‘... (í‰ê·  {rating}ì )")
            reviews = crawl_reviews_by_link(driver, link, max_reviews=max_reviews_per_product)
            all_reviews.extend(reviews)
            time.sleep(2)

        logger.info(f"âœ… ì´ ë¦¬ë·° ìˆ˜ì§‘ ê°œìˆ˜: {len(all_reviews)}")
        return all_reviews

    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

    finally:
        driver.quit()
        logger.debug("ğŸ› ï¸ ë“œë¼ì´ë²„ ì¢…ë£Œ")