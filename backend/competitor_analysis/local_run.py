import threading
import time
from datetime import datetime
import pymysql

from backend.competitor_analysis.crawler import crawl_reviews_by_category
from backend.competitor_analysis.differentiator import summarize_competitor_reviews
from backend.competitor_analysis.competitor_db import insert_review_summary
from backend.competitor_analysis.crawl_signal_server import poll_and_process_signal
from utils.logger import get_logger

logger = get_logger(__name__)

def get_categories_with_last_time(host, user, password, db):
    """
    competitor_review_summary í…Œì´ë¸”ì—ì„œ
    ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìµœì‹  í¬ë¡¤ë§ ì‹œê°„ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        host (str): DB í˜¸ìŠ¤íŠ¸
        user (str): DB ì‚¬ìš©ìëª…
        password (str): DB ë¹„ë°€ë²ˆí˜¸
        db (str): DB ì´ë¦„

    Returns:
        list of tuple: [(category(str), last_crawled_at(datetime)), ...]
    """
    logger.debug("ğŸ› ï¸ ì¹´í…Œê³ ë¦¬ë³„ ìµœì‹  í¬ë¡¤ë§ ì‹œê°„ ì¡°íšŒ ì‹œì‘")
    conn = pymysql.connect(
        host=host,
        user=user,
        password=password,
        db=db,
        port=3306,
        charset='utf8mb4'
    )
    try:
        with conn.cursor() as cursor:
            sql = """
                SELECT category, MAX(crawled_at) as last_crawled_at
                FROM competitor_review_summary
                GROUP BY category
            """
            cursor.execute(sql)
            results = cursor.fetchall()
            logger.debug(f"ğŸ› ï¸ ì¹´í…Œê³ ë¦¬ë³„ ì¡°íšŒ ê²°ê³¼: {results}")
            return results
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ë³„ ìµœì‹  í¬ë¡¤ë§ ì‹œê°„ ì¡°íšŒ ì‹¤íŒ¨: {type(e).__name__}: {e!r}")
        return []
    finally:
        conn.close()
        logger.debug("ğŸ› ï¸ DB ì—°ê²° ì¢…ë£Œ")

def auto_crawl_loop(
    db_config,
    openai_api_key,
    interval=1800
):
    """
    competitor_review_summary í…Œì´ë¸”ì— ì €ì¥ëœ
    ëª¨ë“  ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì£¼ê¸°ì ìœ¼ë¡œ í¬ë¡¤ë§/ìš”ì•½/DB ì €ì¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    (ì¹´í…Œê³ ë¦¬ë³„ ë§ˆì§€ë§‰ ì €ì¥ ì‹œì  ì´í›„ interval(ì´ˆ) ì´ìƒ ê²½ê³¼í•œ ê²½ìš°ë§Œ ì²˜ë¦¬)

    Args:
        db_config (dict): DB ì—°ê²° ì •ë³´
        openai_api_key (str): OpenAI API í‚¤
        interval (int): ì¹´í…Œê³ ë¦¬ë³„ ìµœì†Œ í¬ë¡¤ë§ ê°„ê²©(ì´ˆ)
    """
    logger.info(f"âœ… ìë™ í¬ë¡¤ë§ ë£¨í”„ ì‹œì‘ (ì¹´í…Œê³ ë¦¬ë³„ ìµœì†Œ ì£¼ê¸°: {interval}s)")
    while True:
        logger.debug("ğŸ› ï¸ ì „ì²´ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹œë„")
        categories_with_time = get_categories_with_last_time(
            db_config["host"], db_config["user"], db_config["password"], db_config["db"]
        )
        if not categories_with_time:
            logger.warning("âš ï¸ DBì— ë“±ë¡ëœ ì¹´í…Œê³ ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. 60ì´ˆ í›„ ì¬ì‹œë„")
            time.sleep(60)
            continue

        for category, last_time in categories_with_time:
            logger.debug(f"ğŸ› ï¸ {category} - ë§ˆì§€ë§‰ ì €ì¥ ì‹œê°: {last_time}")
            now = datetime.now()
            # last_timeì´ strì´ë¼ë©´ datetimeìœ¼ë¡œ ë³€í™˜
            if last_time and isinstance(last_time, str):
                try:
                    last_time = datetime.strptime(last_time, "%Y-%m-%d %H:%M:%S")
                except Exception:
                    logger.warning(f"âš ï¸ {category} - last_time íŒŒì‹± ì‹¤íŒ¨, ë¬´ì‹œí•˜ê³  ì§„í–‰")
                    last_time = None

            if last_time is not None and (now - last_time).total_seconds() < interval:
                logger.info(f"â© {category} ìŠ¤í‚µ: ë§ˆì§€ë§‰ ì €ì¥ {last_time}, interval ë¯¸ë„ë‹¬")
                continue

            try:
                logger.debug(f"ğŸ› ï¸ {category} í¬ë¡¤ë§ ì‹œë„")
                reviews = crawl_reviews_by_category(
                    category,
                    max_products=3,
                    max_reviews_per_product=10
                )
                if not reviews:
                    logger.warning(f"âš ï¸ ë¦¬ë·° ì—†ìŒ: {category}")
                    continue
                logger.debug(f"ğŸ› ï¸ {category} ë¦¬ë·° ìš”ì•½ ì‹œë„")
                summary = summarize_competitor_reviews(reviews, openai_api_key)
                logger.debug(f"ğŸ› ï¸ {category} DB ì €ì¥ ì‹œë„")
                insert_review_summary(
                    host=db_config["host"],
                    user=db_config["user"],
                    password=db_config["password"],
                    db=db_config["db"],
                    category=category,
                    review_summary=summary,
                    num_reviews=len(reviews)
                )
                logger.info(f"âœ… ìë™ í¬ë¡¤ë§+ìš”ì•½+DBì €ì¥ ì™„ë£Œ: {category}")
            except Exception as e:
                logger.error(f"âŒ ìë™ í¬ë¡¤ë§/ìš”ì•½/ì €ì¥ ì‹¤íŒ¨: {category}, {type(e).__name__}: {e!r}")
        logger.info(f"ğŸ› ï¸ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì™„ë£Œ, 60ì´ˆ ëŒ€ê¸°")
        time.sleep(60)

def start_local_crawler(
    db_config,
    openai_api_key,
    auto_crawl_interval=1800,
    signal_poll_interval=10
):
    """
    ìë™ í¬ë¡¤ë§ ë£¨í”„ ë° ì‹ í˜¸ polling/ì²˜ë¦¬ ë£¨í”„ë¥¼ ê°ê° ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        db_config (dict): DB ì—°ê²° ì •ë³´
        openai_api_key (str): OpenAI API í‚¤
        auto_crawl_interval (int): ì¹´í…Œê³ ë¦¬ë³„ ìë™ í¬ë¡¤ë§ ê°„ê²©(ì´ˆ)
        signal_poll_interval (int): ì‹ í˜¸ polling ê°„ê²©(ì´ˆ)
    """
    logger.debug("ğŸ› ï¸ ìë™ í¬ë¡¤ë§ ìŠ¤ë ˆë“œ ì‹œì‘")
    t1 = threading.Thread(
        target=auto_crawl_loop,
        args=(db_config, openai_api_key, auto_crawl_interval),
        daemon=True
    )
    t1.start()

    logger.debug("ğŸ› ï¸ ì‹ í˜¸ polling ìŠ¤ë ˆë“œ ì‹œì‘")
    t2 = threading.Thread(
        target=poll_and_process_signal,
        args=(
            db_config["host"],
            db_config["user"],
            db_config["password"],
            db_config["db"],
            openai_api_key,
            signal_poll_interval
        ),
        daemon=True
    )
    t2.start()

    logger.info("âœ… ìë™ í¬ë¡¤ë§/ì‹ í˜¸ polling ìŠ¤ë ˆë“œ ëª¨ë‘ ì‹œì‘")
    while True:
        time.sleep(3600)

if __name__ == "__main__":
    from utils.config import get_openai_api_key, get_db_config

    logger.debug("ğŸ› ï¸ main ì§„ì… í™•ì¸")

    # 1. ì„¤ì •/í‚¤ ë¶ˆëŸ¬ì˜¤ê¸° (utils í•¨ìˆ˜ ê·¸ëŒ€ë¡œ í™œìš©)
    openai_api_key = get_openai_api_key()
    db_config = get_db_config()

    # 2. í¬ë¡¤ëŸ¬ ì‹œì‘
    try:
        start_local_crawler(
            db_config=db_config,
            openai_api_key=openai_api_key,
            auto_crawl_interval=36000,   # 10ì‹œê°„ë§ˆë‹¤ ìë™ ì €ì¥
            signal_poll_interval=5       # 5ì´ˆë§ˆë‹¤ polling
        )
        logger.info("âœ… main ì‹¤í–‰ ì„±ê³µ")
    except Exception as e:
        logger.error(f"âŒ main ì§„ì…/ì‹¤í–‰ ì‹¤íŒ¨: {type(e).__name__}: {e!r}")
        print("main ì˜ˆì™¸:", e)