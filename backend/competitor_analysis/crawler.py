from typing import List
import time
import re
from urllib.parse import quote
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from utils.logger import get_logger

logger = get_logger(__name__)

def clean_coupang_url(url: str) -> str:
    """
    ì¿ íŒ¡ ìƒí’ˆ ìƒì„¸ URLì„ ì •ì œí•´ í‘œì¤€ í˜•íƒœë¡œ ë°˜í™˜í•œë‹¤.

    Args:
        url (str): ì›ë³¸ ì¿ íŒ¡ ìƒí’ˆ ë§í¬.

    Returns:
        str: ì •ì œëœ ìƒí’ˆ ìƒì„¸í˜ì´ì§€ URL.
    """
    logger.debug(f"ğŸ› ï¸ ì¿ íŒ¡ ìƒí’ˆ ë§í¬ ì •ì œ: {url}")
    match = re.search(r"(https://www\.coupang\.com/vp/products/\d+)", url)
    result = match.group(1) if match else url
    logger.debug(f"ğŸ› ï¸ ì •ì œëœ ë§í¬: {result}")
    return result

def init_safe_driver():
    """
    ì…€ë ˆë‹ˆì›€ undetected_chromedriverë¥¼ í¬ë¡¬ ì˜µì…˜ê³¼ í•¨ê»˜ ì´ˆê¸°í™”í•˜ì—¬ ë“œë¼ì´ë²„ ê°ì²´ë¥¼ ë°˜í™˜í•œë‹¤.

    Returns:
        uc.Chrome: ì´ˆê¸°í™”ëœ Selenium í¬ë¡¬ ë“œë¼ì´ë²„ ê°ì²´.
    """
    logger.debug("ğŸ› ï¸ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹œì‘")
    ua = UserAgent()
    user_agent = ua.chrome

    options = uc.ChromeOptions()
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f"user-agent={user_agent}")
    options.binary_location = "C:/Program Files/Google/Chrome/Application/chrome.exe"
    driver = uc.Chrome(options=options)
    logger.info("âœ… ë“œë¼ì´ë²„ ì‹¤í–‰ ì„±ê³µ")
    return driver

def click_review_filter(driver, label: str) -> bool:
    """
    ë¦¬ë·° í•„í„° ë“œë¡­ë‹¤ìš´ì—ì„œ ì§€ì •ëœ ë¼ë²¨ì˜ í•­ëª©ì„ í´ë¦­í•œë‹¤.

    Args:
        driver: Selenium ë“œë¼ì´ë²„ ê°ì²´.
        label (str): í´ë¦­í•  í•„í„° ë¼ë²¨ëª…(ì˜ˆ: "ë‚˜ì¨", "ë³„ë¡œ").

    Returns:
        bool: í´ë¦­ ì„±ê³µ ì—¬ë¶€.
    """
    logger.debug(f"ğŸ› ï¸ ë¦¬ë·° í•„í„° í´ë¦­ ì‹œë„: {label}")
    try:
        dropdown_trigger = WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "div.review-star-search-current-selection"))
        )
        dropdown_trigger.click()
        time.sleep(1)

        WebDriverWait(driver, 5).until(
            EC.visibility_of_element_located((By.CLASS_NAME, "review-star-search-selector"))
        )

        filter_box = driver.find_element(By.CLASS_NAME, "review-star-search-selector")
        items = filter_box.find_elements(By.CSS_SELECTOR, "li")
        found = False

        for item in items:
            try:
                wrapper = item.find_element(By.CLASS_NAME, "review-star-search-item")
                desc_el = wrapper.find_element(By.CLASS_NAME, "review-star-search-item-desc")
                if desc_el.text.strip() == label:
                    item.click()
                    logger.debug(f"ğŸ› ï¸ í•„í„° í´ë¦­ ì„±ê³µ: {label}")
                    time.sleep(2)
                    found = True
                    return True
            except Exception as e:
                logger.warning(f"âš ï¸ í•„í„° í•­ëª© íƒìƒ‰ ì‹¤íŒ¨(ê³„ì† ì§„í–‰): {type(e).__name__}: {e!r}")
                continue

        if not found:
            logger.warning(f"âš ï¸ í•„í„° '{label}' í´ë¦­ ì‹¤íŒ¨ - í•´ë‹¹ ìš”ì†Œ ì—†ìŒ")

    except Exception as e:
        logger.error(f"âŒ í•„í„° í´ë¦­ ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e!r}")

    return False

def crawl_bad_reviews(driver, max_reviews: int) -> List[str]:
    """
    í˜„ì¬ ìƒí’ˆ ìƒì„¸í˜ì´ì§€ì—ì„œ ë¶€ì • ë¦¬ë·°(ë‚˜ì¨/ë³„ë¡œ)ë¥¼ ìµœëŒ€ max_reviewsê°œê¹Œì§€ í¬ë¡¤ë§í•œë‹¤.

    Args:
        driver: Selenium ë“œë¼ì´ë²„ ê°ì²´.
        max_reviews (int): ìµœëŒ€ ìˆ˜ì§‘ ë¦¬ë·° ê°œìˆ˜.

    Returns:
        List[str]: ë¦¬ë·° ë³¸ë¬¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸.
    """
    logger.debug("ğŸ› ï¸ ë¶€ì • ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘")
    reviews = []
    page = 1
    while len(reviews) < max_reviews:
        soup = BeautifulSoup(driver.page_source, "html.parser")
        review_list = soup.select("div.js_reviewArticleListContainer article")
        for article in review_list:
            content_tag = article.select_one("div.sdp-review__article__list__review__content span")
            if content_tag:
                text = content_tag.get_text(strip=True)
                text = re.sub(r"[^\w\sê°€-í£.,!?]", "", text)
                if text:
                    reviews.append(text)
            if len(reviews) >= max_reviews:
                break
        try:
            next_btn = driver.find_element(By.CSS_SELECTOR, "button.sdp-review__article__page__button--next")
            if "disabled" in next_btn.get_attribute("class"):
                break
            next_btn.click()
            page += 1
            time.sleep(1.5)
        except Exception as e:
            logger.debug("ğŸ› ï¸ ë” ì´ìƒ ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ")
            break
    if len(reviews) == 0:
        logger.warning("âš ï¸ ë¶€ì • ë¦¬ë·°ë¥¼ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í•¨")
    else:
        logger.info(f"âœ… ë¶€ì • ë¦¬ë·° {len(reviews)}ê°œ í¬ë¡¤ë§ ì™„ë£Œ")
    return reviews

def crawl_reviews_by_link(driver, url: str, max_reviews: int = 30) -> List[str]:
    """
    ìƒí’ˆ ìƒì„¸í˜ì´ì§€ URLì—ì„œ ë¶€ì • ë¦¬ë·°ë§Œ ìµœëŒ€ max_reviewsê°œê¹Œì§€ í¬ë¡¤ë§í•œë‹¤.

    Args:
        driver: Selenium ë“œë¼ì´ë²„ ê°ì²´.
        url (str): ìƒí’ˆ ìƒì„¸í˜ì´ì§€ URL.
        max_reviews (int): ìµœëŒ€ ìˆ˜ì§‘ ë¦¬ë·° ê°œìˆ˜.

    Returns:
        List[str]: ë¦¬ë·° ë³¸ë¬¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸.
    """
    url = clean_coupang_url(url)
    logger.debug(f"ğŸ› ï¸ ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘: {url}")
    try:
        driver.get(url)
        time.sleep(2)
        reviews = []
        if click_review_filter(driver, "ë‚˜ì¨"):
            reviews = crawl_bad_reviews(driver, max_reviews)
        if len(reviews) < max_reviews and click_review_filter(driver, "ë³„ë¡œ"):
            additional = crawl_bad_reviews(driver, max_reviews - len(reviews))
            reviews.extend(additional)
        if len(reviews) == 0:
            logger.warning(f"âš ï¸ í•´ë‹¹ ìƒí’ˆì—ì„œ ìˆ˜ì§‘ëœ ë¶€ì • ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤: {url}")
        else:
            logger.info(f"âœ… ìˆ˜ì§‘ëœ ë¶€ì • ë¦¬ë·° ìˆ˜: {len(reviews)}")
        return reviews
    except Exception as e:
        logger.error(f"âŒ ë¦¬ë·° ìˆ˜ì§‘ ì‹¤íŒ¨: {type(e).__name__}: {e!r}")
        return []

def crawl_reviews_by_category(
    category: str, max_products: int = 3, max_reviews_per_product: int = 10
) -> List[str]:
    """
    ì¿ íŒ¡ì—ì„œ íŠ¹ì • ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ê²°ê³¼ ìƒìœ„ Nê°œ ìƒí’ˆì— ëŒ€í•´
    ë¶€ì • ë¦¬ë·°ë¥¼ ê°ê° ìµœëŒ€ Mê°œê¹Œì§€ ìˆ˜ì§‘ í›„, ì „ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•œë‹¤.

    Args:
        category (str): ê²€ìƒ‰ í‚¤ì›Œë“œ(ì¹´í…Œê³ ë¦¬).
        max_products (int): ìµœëŒ€ í¬ë¡¤ë§í•  ìƒí’ˆ ê°œìˆ˜.
        max_reviews_per_product (int): ìƒí’ˆë³„ ìµœëŒ€ ë¦¬ë·° ìˆ˜.

    Returns:
        List[str]: ëª¨ë“  ìƒí’ˆì—ì„œ í¬ë¡¤ë§ëœ ë¶€ì • ë¦¬ë·° ë¦¬ìŠ¤íŠ¸.
    """
    logger.debug(f"ğŸ› ï¸ ì¹´í…Œê³ ë¦¬ '{category}'ë¡œ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘")
    all_reviews = []
    driver = init_safe_driver()
    try:
        base_url = f"https://www.coupang.com/np/search?q={quote(category)}&sorter=scoreDesc"
        driver.get(base_url)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(1)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located(
                (By.CSS_SELECTOR, 'ul#product-list li[class^="ProductUnit_productUnit__"]')
            )
        )
        soup = BeautifulSoup(driver.page_source, "html.parser")
        product_links = []
        for li in soup.select('ul#product-list li[class^="ProductUnit_productUnit__"]'):
            a_tag = li.select_one('a[href^="/vp/products/"]')
            if a_tag:
                raw_url = "https://www.coupang.com" + a_tag["href"]
                clean_url = clean_coupang_url(raw_url)
                product_links.append(clean_url)
                logger.debug(f"ğŸ› ï¸ ìƒí’ˆ ë§í¬ ìˆ˜ì§‘: {clean_url}")
            if len(product_links) >= max_products:
                break
        if len(product_links) == 0:
            logger.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìƒí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            logger.info(f"âœ… ì´ ìƒí’ˆ ìˆ˜ì§‘ ê°œìˆ˜: {len(product_links)}")
        for link in product_links:
            logger.debug(f"ğŸ› ï¸ ìƒí’ˆë³„ ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘: {link}")
            reviews = crawl_reviews_by_link(driver, link, max_reviews=max_reviews_per_product)
            all_reviews.extend(reviews)
            time.sleep(1)
        if len(all_reviews) == 0:
            logger.warning("âš ï¸ ì´ ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.info(f"âœ… ì´ ìˆ˜ì§‘ëœ ë¦¬ë·° ìˆ˜: {len(all_reviews)}")
        return all_reviews
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ë¦¬ë·° ìˆ˜ì§‘ ì‹¤íŒ¨: {type(e).__name__}: {e!r}")
        return []
    finally:
        driver.quit()
        logger.debug("ğŸ› ï¸ ë“œë¼ì´ë²„ ì¢…ë£Œ")