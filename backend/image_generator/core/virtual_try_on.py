import datetime
import os
import torch
from PIL import Image
from diffusers import AutoPipelineForInpainting, AutoencoderKL, ControlNetModel
from controlnet_aux import MidasDetector
from diffusers.utils import load_image
from huggingface_hub import snapshot_download
from utils.logger import get_logger

logger = get_logger(__name__)

class VirtualTryOnPipeline:
    def __init__(self, model_dir="./backend/models"):
        """
        ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        ëª¨ë“  ëª¨ë¸ì€ "./backend/models" ê²½ë¡œì— ë‹¤ìš´ë¡œë“œë˜ê±°ë‚˜ ë¡œë“œë¨
        """
        logger.debug("ğŸ› ï¸ VirtualTryOnPipeline ì´ˆê¸°í™” ì‹œì‘")
        self.model_dir = model_dir
        os.makedirs(self.model_dir, exist_ok=True)

        self.pipeline = None
        self.vae = None
        self.controlnet = None
        self.midas_detector = None
        logger.info(f"âœ… VirtualTryOnPipeline ì´ˆê¸°í™” ì™„ë£Œ. ëª¨ë¸ì€ '{self.model_dir}'ì—ì„œ ë¡œë“œ/ì €ì¥")
        self._load_models()

    def _download_and_load_models(self, model_name_or_path, subfolder=None, model_class=None, **kwargs):
        """
        ëª¨ë¸ì„ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜ ë¡œë“œí•©ë‹ˆë‹¤.
        kwargsëŠ” from_pretrainedì— ì „ë‹¬í•œ ì¶”ê°€ ì¸ìì…ë‹ˆë‹¤.
        """
        local_path = os.path.join(self.model_dir, model_name_or_path.replace("/", "_"))  # ë¡œì»¬ í´ë”ëª… ìƒì„±

        # ë‹¤ìš´ë¡œë“œ í™•ì¸
        if not os.path.exists(local_path):
            logger.debug(f"ğŸ› ï¸ {model_name_or_path} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            snapshot_download(repo_id=model_name_or_path, local_dir=local_path)
            logger.info(f"âœ… {model_name_or_path} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_path}")
        else:
            logger.info(f"âœ… {model_name_or_path} ëª¨ë¸ì´ ì´ë¯¸ ì¡´ì¬: {local_path}")
        
        if model_class:
            if subfolder: # íŠ¹ì • ì„œë¸Œí´ë”ê°€ ìˆëŠ” ê²½ìš°
                return model_class.from_pretrained(os.path.join(local_path, subfolder), **kwargs)
            else: # ì„œë¸Œí´ë” ì—†ì´ ë°”ë¡œ ë¡œë“œí•˜ëŠ” ê²½ìš°
                return model_class.from_pretrained(local_path, **kwargs)
        return local_path

    def _load_models(self):
        """ëª¨ë“  í•„ìš”í•œ ëª¨ë¸ë“¤ì„ ë¡œì»¬ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ë‹¤ìš´ë¡œë“œ í•©ë‹ˆë‹¤."""
        logger.debug("ğŸ› ï¸ VAE ë¡œë”© ì‹œì‘")
        vae_path = self._download_and_load_models(
            "madebyollin/sdxl-vae-fp16-fix",
            model_class=AutoencoderKL,
            torch_dtype=torch.float16
        )
        self.vae = vae_path
        logger.info("âœ… VAE ë¡œë”© ì™„ë£Œ")

        logger.debug("ğŸ› ï¸ Depth Detector ë¡œë”© ì‹œì‘")
        controlnet_aux_repo_path = self._download_and_load_models("lllyasviel/ControlNet")
        midas_model_dir_for_detector = os.path.join(controlnet_aux_repo_path, "annotator", "ckpts")
        self.midas_detector = MidasDetector.from_pretrained(midas_model_dir_for_detector, model_type="dpt_hybrid")
        logger.info("âœ… Depth Detector ë¡œë”© ì™„ë£Œ")

        logger.debug("ğŸ› ï¸ ControlNet (Depth) ë¡œë”© ì‹œì‘")
        controlnet_depth_path = self._download_and_load_models(
            "diffusers/controlnet-depth-sdxl-1.0",
            model_class=ControlNetModel,
            torch_dtype=torch.float16
        )
        self.controlnet = controlnet_depth_path
        logger.info("âœ… ControlNet (Depth) ë¡œë”© ì™„ë£Œ")

        logger.debug("ğŸ› ï¸ Pipeline ë¡œë”© ì‹œì‘")
        self.pipeline = AutoPipelineForInpainting.from_pretrained(
            "diffusers/stable-diffusion-xl-1.0-inpainting-0.1",
            vae=self.vae, # ë¡œë“œëœ VAE ê°ì²´ ì „ë‹¬
            controlnet=self.controlnet, # ë¡œë“œëœ ControlNet ê°ì²´ ì „ë‹¬
            torch_dtype=torch.float16,
            variant="fp16",
            use_safetensors=True
        ).to("cuda")
        logger.info("âœ… Pipeline ë¡œë”© ì™„ë£Œ")

        logger.debug("ğŸ› ï¸ IP-Adapter ëª¨ë¸ ë¡œë”© ì‹œì‘")
        ip_adapter_repo_path = self._download_and_load_models("h94/IP-Adapter")
        ip_adapter_model_path = os.path.join(ip_adapter_repo_path, "sdxl_models", "ip-adapter_sdxl.bin")
        self.pipeline.load_ip_adapter(
            ip_adapter_repo_path,
            subfolder="sdxl_models",
            weight_name="ip-adapter_sdxl.bin",
            low_cpu_mem_usage=True
        )
        self.pipeline.set_ip_adapter_scale(2.0)
        logger.info("âœ… IP-Adapter ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        logger.debug("ğŸ› ï¸ LoRA ë¡œë”© ì‹œì‘")
        lora_path = self._download_and_load_models(
            "Norod78/weird-fashion-show-outfits-sdxl-lora",
            model_class=None,
        )
        self.pipeline.load_lora_weights(lora_path, weight_name='sdxl-WeirdOutfit-Dreambooh.safetensors')
        logger.info("âœ… LoRA ë¡œë”© ì™„ë£Œ")
        logger.info("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë”© ë° ì¤€ë¹„ ì™„ë£Œ.")

    def try_on(self, image_path, ip_image_path, mask_image_path, prompt, negative_prompt,
               width=512, height=768, controlnet_conditioning_scale=0.7,
               strength=0.99, guidance_scale=7.5, num_inference_steps=100, seed=None):
        """
        ê°€ìƒ í”¼íŒ…ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        logger.debug("ğŸ› ï¸ ì…ë ¥ ì´ë¯¸ì§€ ë¡œë”©")
        image = load_image(image_path).convert("RGB")
        ip_image = load_image(ip_image_path).convert("RGB")
        mask_image = load_image(mask_image_path)

        logger.debug("ğŸ› ï¸ ControlNet ì œì–´ ì´ë¯¸ì§€ ìƒì„±")
        control_image_depth = self.midas_detector(image)

        if seed is None:
            now = datetime.datetime.now()
            seed = int(now.strftime("%Y%m%d%H%M%S"))
        generator = torch.manual_seed(seed)
        logger.debug(f"ğŸ› ï¸ ë‚ ì§œ ì‹œë“œ: {seed}")

        logger.debug(f"ğŸ› ï¸ ì´ë¯¸ì§€ ìƒì„± ì‹œì‘")
        final_image = self.pipeline(
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            image=image,
            mask_image=mask_image,
            ip_adapter_image=ip_image,
            control_image=control_image_depth,
            controlnet_conditioning_scale=controlnet_conditioning_scale,
            strength=strength,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
            generator=generator,
        ).images[0]
        logger.info(f"âœ… ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
        return final_image


# try: 
#     logger.debug("ğŸ› ï¸ ì´ë¯¸ì§€ ë¡œë”©")
#     image = load_image('/home/user/2025-GEO-Project/backend/data/output/model_removed_bg.png').convert("RGB")
#     ip_image = load_image('/home/user/2025-GEO-Project/backend/data/output/greendress_removed_bg.png').convert("RGB")
#     mask_image= load_image('/home/user/2025-GEO-Project/backend/data/input/model_mask3.png')

#     logger.debug("ğŸ› ï¸ Depth Detector ë¡œë”©")
#     midas_detector = MidasDetector.from_pretrained("lllyasviel/ControlNet")
#     control_image_depth = midas_detector(image)

#     logger.debug("ğŸ› ï¸ VAE ë¡œë”©")
#     vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)

#     logger.debug("ğŸ› ï¸ ControlNet ë¡œë”©")
#     controlnet = ControlNetModel.from_pretrained(
#         "diffusers/controlnet-depth-sdxl-1.0",
#         torch_dtype=torch.float16
#     )

#     logger.debug("ğŸ› ï¸ Pipeline ë¡œë”©")
#     pipeline = AutoPipelineForInpainting.from_pretrained(
#         "diffusers/stable-diffusion-xl-1.0-inpainting-0.1",
#         vae=vae,
#         torch_dtype=torch.float16,
#         variant="fp16",
#         use_safetensors=True
#     ).to("cuda")

#     logger.debug("ğŸ› ï¸ ControlNet ì£¼ì…")
#     pipeline.controlnet = controlnet

#     logger.debug("ğŸ› ï¸ IP-Adapter ì£¼ì…")
#     pipeline.load_ip_adapter("h94/IP-Adapter", subfolder="sdxl_models", weight_name="ip-adapter_sdxl.bin", low_cpu_mem_usage=True)
#     pipeline.set_ip_adapter_scale(2.0)

#     logger.debug("ğŸ› ï¸ LoRA ì£¼ì…")
#     pipeline.load_lora_weights('Norod78/weird-fashion-show-outfits-sdxl-lora', weight_name='sdxl-WeirdOutfit-Dreambooh.safetensors')

#     now = datetime.datetime.now()
#     seed = int(now.strftime("%Y%m%d%H%M%S"))
#     generator = torch.manual_seed(seed)
#     logger.debug(f"ğŸ› ï¸ ë‚ ì§œ ì‹œë“œ: {seed}")
    
#     logger.debug(f"ğŸ› ï¸ ì´ë¯¸ì§€ ìƒì„± ì‹œì‘")
#     final_image = pipeline(
#         prompt="photorealistic, perfect body, beautiful skin, realistic skin, natural skin, a man wearing blue suit.",
#         negative_prompt="ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet, deformed face, deformed clothing, deformed skin, bad skin, leggings, tights, stockings",
#         width=512,
#         height=768,
#         image=image,
#         mask_image=mask_image,
#         ip_adapter_image=ip_image,
#         control_image=control_image_depth,
#         controlnet_conditioning_scale=0.7,
#         strength=0.99,
#         guidance_scale=7.5,
#         num_inference_steps=100,
#         generator=generator,
#     ).images[0]
#     logger.info(f"âœ… ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
#     final_image.save("final_image.png")
# except Exception as e:
#     logger.warning(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")