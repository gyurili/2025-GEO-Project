import os
import torch
from dotenv import load_dotenv
from openai import OpenAI
from transformers import AutoTokenizer
from backend.models.model_handler import get_model_pipeline
from backend.text_generator.cleaner import clean_response
from backend.text_generator.prompt_builder import *
from backend.text_generator.prompt_builder_hf import system_instruction, css_friendly_prompt
from utils.logger import get_logger

load_dotenv()
logger = get_logger(__name__)

# OpenAI
def generate_openai(product: dict) -> dict:
    """
    ì œê³µëœ ì œí’ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ OpenAIë¥¼ í†µí•´ ìƒì„¸í˜ì´ì§€ HTMLì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        product (dict): ìƒì„¸í˜ì´ì§€ ìƒì„±ì— í•„ìš”í•œ ì œí’ˆ ì •ë³´ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬

    Returns:
        dict: ìƒì„±ëœ ìƒì„¸í˜ì´ì§€ HTMLì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
    """
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    prompt_parts = [
        apply_schema_prompt(product),
        natural_tone_prompt(),
        keyword_variation_prompt(),
        html_structure_prompt(),
        qna_format_prompt(),
        quantitative_prompt(),
        expert_quote_prompt(),
        storytelling_prompt(),
        modern_design_prompt(),
    ]

    if product.get("differences"):
        diff_prompt = "\n".join([
            "ì´ ìƒí’ˆì€ ê²½ìŸì‚¬ ëŒ€ë¹„ ë‹¤ìŒê³¼ ê°™ì€ ì°¨ë³„ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:",
            *[f"- {item}" for item in product["differences"]],
            "ìœ„ ì°¨ë³„ì ë“¤ì„ ìƒì„¸í˜ì´ì§€ ë‚´ìš©ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ê°•ì¡°í•´ ì£¼ì„¸ìš”."
        ])
        prompt_parts.insert(1, diff_prompt)
    
    prompt_parts += [
        "ìƒì„¸í˜ì´ì§€ ë‚´ì— ìƒí’ˆ ì´ë¯¸ì§€ë¥¼ í¬í•¨ì‹œì¼œì£¼ì„¸ìš”.",
        f'ìƒí’ˆ ì´ë¯¸ì§€ëŠ” product["vton_image_path"] ê²½ë¡œì— ìˆìŠµë‹ˆë‹¤.'
        f'ë”°ë¼ì„œ íƒœê·¸ëŠ” ë‹¤ìŒ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”. <img class="product-image" src="{product["vton_image_path"]}" alt=" ">',
        "alt ì†ì„±ì€ ì œí’ˆëª…ê³¼ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ìë™ ìƒì„±í•´ì£¼ì„¸ìš”.",
        "ëª¨ë“  ì •ë³´ë¥¼ HTMLë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”. ê²°ê³¼ëŠ” <html> ~ </html> íƒœê·¸ ì•ˆì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
    ]
    
    prompt = "\n".join(prompt_parts)
    logger.info("ğŸ› ï¸ OpenAI ìš”ì²­ ì‹œì‘")

    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.9
    )
    logger.info("âœ… OpenAI API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")

    html_text = response.choices[0].message.content
    html_text = clean_response(html_text)
    logger.info("âœ… ì½”ë“œ ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì œê±° ì™„ë£Œ")
    
    return {"html_text": html_text}


# HuggingFace
hf_model = None
hf_tokenizer = None

def load_hf_model():
    model_id = "Markr-AI/Gukbap-Qwen2.5-7B"
    lora_path = "backend/models/adapter"
    model = get_model_pipeline(
                model_id=model_id,
                model_type="casual_lm",
                use_4bit=True,
                lora_path=lora_path,
                use_ip_adapter=False,
                save_dir="/home/spai0103/2025-GEO-Project/backend/models"
            )
    tokenizer = AutoTokenizer.from_pretrained(lora_path, use_fast=False, local_files_only=True)
    return model, tokenizer

def generate_hf(product: dict) -> dict:
    global hf_model, hf_tokenizer
    
    if hf_model is None:
        logger.info("ğŸ› ï¸ HuggingFace ëª¨ë¸ ë¡œë”© ì¤‘")
        hf_model, hf_tokenizer = load_hf_model()
        
    prompt_parts = [
        system_instruction(product).strip(),
        css_friendly_prompt().strip(),
        "ë‹¤ìŒì€ ì œí’ˆ ìƒì„¸í˜ì´ì§€ HTMLì…ë‹ˆë‹¤.",
        f'ì´ë¯¸ì§€ íƒœê·¸ëŠ” ë‹¤ìŒ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”. <img class="product-image" src="{product["vton_image_path"]}" alt=" ">',
        "alt ì†ì„±ì€ ì œí’ˆëª…ê³¼ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ìë™ ìƒì„±í•´ì£¼ì„¸ìš”.",
        "<!DOCTYPE html>"
    ]
    
    if product.get("differences"):
        diff_prompt = "\n".join([
            "ì´ ìƒí’ˆì€ ë‹¤ìŒê³¼ ê°™ì€ ì°¨ë³„ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:",
            *[f"- {item}" for item in product["differences"]],
            "ìœ„ ì°¨ë³„ì ë“¤ì„ ìƒì„¸í˜ì´ì§€ ë‚´ìš©ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ê°•ì¡°í•´ ì£¼ì„¸ìš”."
        ])
        prompt_parts.insert(1, diff_prompt)

    prompt = "\n".join(prompt_parts)

    logger.info("ğŸ› ï¸ HuggingFace ìš”ì²­ ì‹œì‘")
    inputs = hf_tokenizer(prompt, return_tensors="pt")
    input_ids = inputs["input_ids"].to(hf_model.device)
    attention_mask = inputs["attention_mask"].to(hf_model.device)

    with torch.no_grad():
        output_ids = hf_model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=2048,
            do_sample=True,
            temperature=0.9,
            top_p=0.95,
            repetition_penalty=1.1
        )

    output_text = hf_tokenizer.decode(output_ids[0], skip_special_tokens=True)
    logger.info("âœ… HuggingFace ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
    html_text = clean_response(output_text, strict=True)
    logger.info("âœ… ì½”ë“œ ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì œê±° ì™„ë£Œ")
    
    return {"html_text": html_text}