import os
import torch
from dotenv import load_dotenv
from openai import OpenAI
from transformers import AutoTokenizer
from backend.models.model_handler import get_model_pipeline
from backend.text_generator.cleaner import clean_response
from backend.text_generator.prompt_builder import *
from backend.text_generator.prompt_builder_hf import system_instruction, css_friendly_prompt
from utils.logger import get_logger
from utils.config import get_openai_api_key

load_dotenv()
logger = get_logger(__name__)

# OpenAI
def generate_openai(product: dict) -> dict:
    """
    ì œê³µëœ ì œí’ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ OpenAIë¥¼ í†µí•´ ìƒì„¸í˜ì´ì§€ HTMLì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        product (dict): ìƒì„¸í˜ì´ì§€ ìƒì„±ì— í•„ìš”í•œ ì œí’ˆ ì •ë³´ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬

    Returns:
        dict: ìƒì„±ëœ ìƒì„¸í˜ì´ì§€ HTMLì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
    """
    client = OpenAI(api_key=get_openai_api_key())
    
    prompt_parts = [
        apply_schema_prompt(product),
        natural_tone_prompt(),
        keyword_variation_prompt(),
        html_structure_prompt(),
        qna_format_prompt(),
        quantitative_prompt(),
        expert_quote_prompt(),
        storytelling_prompt(),
        modern_design_prompt(),
    ]

    # ì°¨ë³„ì  ë°˜ì˜
    if product.get("differences"):
        diff_prompt = "\n".join([
            "ì´ ìƒí’ˆì€ ê²½ìŸì‚¬ ëŒ€ë¹„ ë‹¤ìŒê³¼ ê°™ì€ ì°¨ë³„ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:",
            *[f"- {item}" for item in product["differences"]],
            "ìœ„ ì°¨ë³„ì ë“¤ì„ ìƒì„¸í˜ì´ì§€ ë‚´ìš©ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ê°•ì¡°í•´ ì£¼ì„¸ìš”."
        ])
        prompt_parts.insert(1, diff_prompt)
        
    # ì´ë¯¸ì§€ ë°˜ì˜
    image_prompt_lines = []
    image_paths = product.get("image_path_list", [])

    if image_paths:
        image_prompt_lines = ["ìƒì„¸í˜ì´ì§€ ë‚´ì— ë‹¤ìŒ ìƒí’ˆ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ í¬í•¨ì‹œì¼œì£¼ì„¸ìš”:"]
        for path in image_paths:
            image_prompt_lines.append(f'- <img class="product-image" src="{path}" alt="...">')
        image_prompt_lines.append("ê° ì´ë¯¸ì§€ì˜ alt ì†ì„±ì€ ì œí’ˆëª…ê³¼ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ìë™ ìƒì„±í•´ì£¼ì„¸ìš”")
        image_prompt_lines.append("ê° ì´ë¯¸ì§€ëŠ” ì ì ˆí•œ ì„¹ì…˜ì— ë¶„ì‚°í•˜ì—¬ ë°°ì¹˜í•´ì£¼ì„¸ìš”.")

    prompt_parts += image_prompt_lines
    prompt_parts.append("ëª¨ë“  ì •ë³´ë¥¼ HTMLë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”. ê²°ê³¼ëŠ” <html> ~ </html> íƒœê·¸ ì•ˆì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤")
    
    prompt = "\n".join(prompt_parts)
    logger.info("ğŸ› ï¸ OpenAI ìš”ì²­ ì‹œì‘")

    try:
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.9
        )
        logger.info("âœ… OpenAI API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
    except Exception as e:
        raise RuntimeError(f"âŒ OpenAI API ìš”ì²­ ì‹¤íŒ¨: {e}")

    html_text = response.choices[0].message.content
    html_text = clean_response(html_text)
    logger.info("âœ… ì½”ë“œ ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì œê±° ì™„ë£Œ")
    
    return {"html_text": html_text}


# HuggingFace
hf_model = None
hf_tokenizer = None

def load_hf_model():
    """
    HuggingFace ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œì»¬ì—ì„œ ë¡œë”©í•©ë‹ˆë‹¤.

    Returns:
        tuple: (pipeline ëª¨ë¸ ê°ì²´, í† í¬ë‚˜ì´ì € ê°ì²´)
    """
    model_id = "Markr-AI/Gukbap-Qwen2.5-7B"
    lora_path = "backend/models/adapter"
    model = get_model_pipeline(
                model_id=model_id,
                model_type="casual_lm",
                use_4bit=True,
                lora_path=lora_path,
                use_ip_adapter=False,
                save_dir="/home/spai0103/2025-GEO-Project/backend/models"
            )
    tokenizer = AutoTokenizer.from_pretrained(lora_path, use_fast=False, local_files_only=True)
    return model, tokenizer

def generate_hf(product: dict) -> dict:
    """
    ì œê³µëœ ì œí’ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ HuggingFaceë¥¼ í†µí•´ ìƒì„¸í˜ì´ì§€ HTMLì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        product (dict): ìƒì„¸í˜ì´ì§€ ìƒì„±ì— í•„ìš”í•œ ì œí’ˆ ì •ë³´ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬

    Returns:
        dict: ìƒì„±ëœ ìƒì„¸í˜ì´ì§€ HTMLì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
    """
    global hf_model, hf_tokenizer
    
    if hf_model is None:
        try:
            logger.info("ğŸ› ï¸ HuggingFace ëª¨ë¸ ë¡œë”© ì¤‘")
            hf_model, hf_tokenizer = load_hf_model()
            logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            raise RuntimeError(f"HuggingFace ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        
    prompt_parts = [
        system_instruction(product).strip(),
        css_friendly_prompt().strip(),
    ]
    
    # ì°¨ë³„ì  ë°˜ì˜
    if product.get("differences"):
        diff_prompt = "\n".join([
            "ì´ ìƒí’ˆì€ ë‹¤ìŒê³¼ ê°™ì€ ì°¨ë³„ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:",
            *[f"- {item}" for item in product["differences"]],
            "ìœ„ ì°¨ë³„ì ë“¤ì„ ìƒì„¸í˜ì´ì§€ ë‚´ìš©ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ê°•ì¡°í•´ ì£¼ì„¸ìš”."
        ])
        prompt_parts.insert(1, diff_prompt)
        
    # ì´ë¯¸ì§€ ë°˜ì˜
    image_prompt_lines = []
    image_paths = product.get("image_path_list", [])

    if image_paths:
        image_prompt_lines = ["ìƒì„¸í˜ì´ì§€ ë‚´ì— ë‹¤ìŒ ìƒí’ˆ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ í¬í•¨ì‹œì¼œì£¼ì„¸ìš”:"]
        for path in image_paths:
            image_prompt_lines.append(f'- <img class="product-image" src="{path}" alt="...">')
        image_prompt_lines.append("ê° ì´ë¯¸ì§€ì˜ alt ì†ì„±ì€ ì œí’ˆëª…ê³¼ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ìë™ ìƒì„±í•´ì£¼ì„¸ìš”")
        image_prompt_lines.append("ê° ì´ë¯¸ì§€ëŠ” ì ì ˆí•œ ì„¹ì…˜ì— ë¶„ì‚°í•˜ì—¬ ë°°ì¹˜í•´ì£¼ì„¸ìš”.")

    prompt_parts += image_prompt_lines
    prompt_parts += [
        "ëª¨ë“  ì •ë³´ë¥¼ HTMLë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”. ê²°ê³¼ëŠ” <html> ~ </html> íƒœê·¸ ì•ˆì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤",
        "ë‹¤ìŒì€ ì œí’ˆ ìƒì„¸í˜ì´ì§€ HTMLì…ë‹ˆë‹¤.",
        "<!DOCTYPE html>"
    ]

    prompt = "\n".join(prompt_parts)

    logger.info("ğŸ› ï¸ HuggingFace ìš”ì²­ ì‹œì‘")
    try:
        inputs = hf_tokenizer(prompt, return_tensors="pt", truncation=True)
        input_ids = inputs["input_ids"].to(hf_model.device)
        attention_mask = inputs["attention_mask"].to(hf_model.device)

        with torch.no_grad():
            output_ids = hf_model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=2048,
                do_sample=True,
                temperature=0.9,
                top_p=0.95,
                repetition_penalty=1.1
            )

        output_text = hf_tokenizer.decode(output_ids[0], skip_special_tokens=True)
        logger.info("âœ… HuggingFace ìƒì„¸í˜ì´ì§€ ìƒì„± ì™„ë£Œ")
    except Exception as e:
        raise RuntimeError(f"HuggingFace ìƒì„¸í˜ì´ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
    
    html_text = clean_response(output_text, strict=True)
    logger.info("âœ… ì½”ë“œ ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì œê±° ì™„ë£Œ")
    
    return {"html_text": html_text}