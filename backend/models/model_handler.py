import os
import torch
from dotenv import load_dotenv
from diffusers import DiffusionPipeline, AutoPipelineForText2Image
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import snapshot_download
from utils.logger import get_logger

logger = get_logger(__name__)

load_dotenv()

MODEL_LOADERS = {
    "diffusion": AutoPipelineForText2Image,
    "casual_lm": AutoModelForCausalLM,
    "encoder": AutoModel,
}

def download_model(
        model_id: str, 
        model_type: str = "diffusion",
        save_dir: str = "/home/user/2025-GEO-Project/backend/models"
    ):
    """
    Hugging Faceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì§€ì •ëœ ê²½ë¡œì— ì €ì¥
    ëª¨ë¸ ìœ í˜•ì— ë”°ë¼ í™•ì¥ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„
    CPU/GPU í™˜ê²½ì— ë”°ë¼ torch_dtypeì„ ì„¤ì •

    Args:
        model_id (str): Hugging Face ëª¨ë¸ ID (ì˜ˆ: "stabilityai/stable-diffusion-xl-base-1.0").
        model_type (str): ëª¨ë¸ ìœ í˜• (ì˜ˆ: "diffusion", "causal_lm", "encoder" ë“±).
        save_dir (str, optional): ëª¨ë¸ì„ ì €ì¥í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
                                  ê¸°ë³¸ê°’ì€ "/home/user/2025-GEO-Project/backend/models".

    Returns:
        str: ëª¨ë¸ì´ ì €ì¥ëœ ìµœì¢… ê²½ë¡œ. ë‹¤ìš´ë¡œë“œ ë˜ëŠ” ì €ì¥ì— ì‹¤íŒ¨í•˜ë©´ None ë°˜í™˜.
    """
    if model_type not in MODEL_LOADERS:
        logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ìœ í˜•: {model_type}")
        return None

    model_name_for_path = model_id.split("/")[-1]
    model_save_path = os.path.join(save_dir, model_name_for_path)

    if os.path.exists(model_save_path) and os.listdir(model_save_path):
        logger.info(f"âœ… ëª¨ë¸ {model_id}ì´(ê°€) ì´ë¯¸ {save_dir}ì— ì¡´ì¬")
        return model_save_path

    logger.debug(f"ğŸ› ï¸ ëª¨ë¸ {model_id}ë¥¼ {save_dir}ì— ë‹¤ìš´ë¡œë“œ ì‹œì‘")

    token = os.getenv("HF_TOKEN")
    if token is None:
        logger.warning("âš ï¸ Hugging Face API í† í°(HF_TOKEN)ì´ .envì— ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # GPUì²´í¬
    load_kwargs = {}
    if torch.cuda.is_available():
        load_kwargs["torch_dtype"] = torch.float16
        logger.info("âœ… GPUë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ")
    else: 
        load_kwargs["torch_dtype"] = torch.float32
        logger.info("âœ… CPUë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ")

    if model_type == "diffusion":
        pass
    else:
        load_kwargs["device_map"] = "auto"

    try:
        loader_cls = MODEL_LOADERS[model_type]
        model = loader_cls.from_pretrained(model_id, token=token, **load_kwargs)

        model.save_pretrained(model_save_path)
        logger.info(f"âœ… ëª¨ë¸ '{model_id}'ì´(ê°€) '{model_save_path}'ì— ì €ì¥")
        return model_save_path

    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ '{model_id}' ë‹¤ìš´ë¡œë“œ ë° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def load_model(
        model_path: str,
        model_type: str = "diffusion"
    ):
    """
    ì €ì¥ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

    Args:
        model_path (str): ì‚¬ì „ì— ì €ì¥ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        model_type (str): ëª¨ë¸ ìœ í˜• ("diffusion", "causal_lm", "encoder" ë“±)

    Returns:
        model: ë¡œë“œëœ ëª¨ë¸ ê°ì²´. ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
    """
    if not os.path.exists(model_path):
        logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ '{model_path}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

    if model_type not in MODEL_LOADERS:
        logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” model_type: {model_type}")
        return None

    load_kwargs = {}
    if torch.cuda.is_available():
        load_kwargs["torch_dtype"] = torch.float16
        logger.info("âœ… GPUë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œ")
    else: 
        load_kwargs["torch_dtype"] = torch.float32
        logger.info("âœ… CPUë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œ")

    if model_type == "diffusion":
        load_kwargs["device_map"] = "balanced"
    else:
        load_kwargs["device_map"] = "auto"

    try:
        model = MODEL_LOADERS[model_type].from_pretrained(model_path, **load_kwargs)
        logger.info(f"âœ… ëª¨ë¸ì´ '{model_path}'ì—ì„œ ë¡œë“œ")
        return model
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def get_model_pipeline(
        model_id: str,
        model_type: str = "diffusion",
        use_ip_adapter: bool = True,
        ip_adapter_config: dict = None,
    ):
    model_path = download_model(model_id, model_type)
    if model_path is None:
        logger.error("âŒ ëª¨ë¸ ê²½ë¡œê°€ Noneì…ë‹ˆë‹¤. ë¡œë”©ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return None

    model_pipeline = load_model(model_path, model_type)

    # IP-Adapter ì£¼ì… (ì˜µì…˜)
    if use_ip_adapter and not hasattr(model_pipeline, "image_proj_model"):
        try:
            adapter_config = ip_adapter_config or {
                "repo_id": "h94/IP-Adapter",
                "subfolder": "sdxl_models",
                "weight_name": "ip-adapter_sdxl.bin",
                "scale": 0.8,
            }
            model_pipeline.load_ip_adapter(
                adapter_config["repo_id"],
                subfolder=adapter_config["subfolder"],
                weight_name=adapter_config["weight_name"],
            )
            model_pipeline.set_ip_adapter_scale(adapter_config["scale"])
            logger.info("âœ… IP-Adapter ìë™ ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ IP-Adapter ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
            
    return model_pipeline