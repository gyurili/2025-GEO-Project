import os
import torch
from dotenv import load_dotenv
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM
from diffusers import (
    AutoPipelineForInpainting,
    AutoPipelineForText2Image,
    ControlNetModel,
    StableDiffusionPipeline,
    AutoencoderKL
)

from utils.logger import get_logger

logger = get_logger(__name__)

load_dotenv()

MODEL_LOADERS = {
    "diffusion_pipeline": AutoPipelineForInpainting,
    "diffusion_text2img": AutoPipelineForText2Image,
    "vae": AutoencoderKL,
    "controlnet": ControlNetModel,
    "casual_lm": AutoModelForCausalLM,
    "encoder": AutoModel,
}

def download_model(
        model_id: str, 
        model_type: str = "diffusion_text2img",
        save_dir: str = "/home/user/2025-GEO-Project/backend/models"
    ):
    """
    Hugging Faceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì§€ì •ëœ ê²½ë¡œì— ì €ì¥
    ëª¨ë¸ ìœ í˜•ì— ë”°ë¼ í™•ì¥ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„
    CPU/GPU í™˜ê²½ì— ë”°ë¼ torch_dtypeì„ ì„¤ì •

    Args:
        model_id (str): Hugging Face ëª¨ë¸ ID (ì˜ˆ: "stabilityai/stable-diffusion-xl-base-1.0").
        model_type (str): ëª¨ë¸ ìœ í˜• (ì˜ˆ: "diffusion_text2img", "causal_lm", "encoder" ë“±).
        save_dir (str, optional): ëª¨ë¸ì„ ì €ì¥í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
                                  ê¸°ë³¸ê°’ì€ "/home/user/2025-GEO-Project/backend/models".

    Returns:
        str: ëª¨ë¸ì´ ì €ì¥ëœ ìµœì¢… ê²½ë¡œ. ë‹¤ìš´ë¡œë“œ ë˜ëŠ” ì €ì¥ì— ì‹¤íŒ¨í•˜ë©´ None ë°˜í™˜.
    """
    if model_type not in MODEL_LOADERS:
        logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ìœ í˜•: {model_type}")
        return None

    model_name_for_path = model_id.split("/")[-1]
    model_save_path = os.path.join(save_dir, model_name_for_path)

    if os.path.exists(model_save_path) and os.listdir(model_save_path):
        logger.info(f"âœ… ëª¨ë¸ {model_id}ì´(ê°€) ì´ë¯¸ {save_dir}ì— ì¡´ì¬")
        return model_save_path

    logger.debug(f"ğŸ› ï¸ ëª¨ë¸ {model_id}ë¥¼ {save_dir}ì— ë‹¤ìš´ë¡œë“œ ì‹œì‘")

    token = os.getenv("HF_TOKEN")
    if token is None:
        logger.warning("âš ï¸ Hugging Face API í† í°(HF_TOKEN)ì´ .envì— ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # GPUì²´í¬
    load_kwargs = {}
    if torch.cuda.is_available():
        load_kwargs["torch_dtype"] = torch.float16
        logger.info("âœ… GPUë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ")
    else: 
        load_kwargs["torch_dtype"] = torch.float32
        logger.info("âœ… CPUë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ")

    try:
        loader_cls = MODEL_LOADERS[model_type]
        model = loader_cls.from_pretrained(model_id, token=token, **load_kwargs)
        model.save_pretrained(model_save_path)
        logger.info(f"âœ… {model_type} ëª¨ë¸ '{model_id}' ì €ì¥ ì™„ë£Œ: {model_save_path}")
        return model_save_path

    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({model_id}): {e}")
        return None


def load_model(
        model_path: str,
        model_type: str = "diffusion_text2img"
    ):
    """
    ì €ì¥ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

    Args:
        model_path (str): ì‚¬ì „ì— ì €ì¥ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        model_type (str): ëª¨ë¸ ìœ í˜• ("diffusion_text2img", "causal_lm", "encoder" ë“±)

    Returns:
        model: ë¡œë“œëœ ëª¨ë¸ ê°ì²´. ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
    """
    if not os.path.exists(model_path):
        logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ '{model_path}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

    if model_type not in MODEL_LOADERS:
        logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” model_type: {model_type}")
        return None

    load_kwargs = {}
    if torch.cuda.is_available():
        load_kwargs["torch_dtype"] = torch.float16
        logger.info("âœ… GPUë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œ")
    else: 
        load_kwargs["torch_dtype"] = torch.float32
        logger.info("âœ… CPUë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œ")

    if model_type == "diffusion_text2img" or model_type == "diffusion_pipeline":
        load_kwargs["device_map"] = "balanced"
    elif model_type == "controlnet":
        load_kwargs["device_map"] = "cuda"
    else:
        load_kwargs["device_map"] = "auto"

    try:
        model = MODEL_LOADERS[model_type].from_pretrained(model_path, **load_kwargs)
        logger.info(f"âœ… ëª¨ë¸ì´ '{model_path}'ì—ì„œ ë¡œë“œ")
        return model
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def get_model_pipeline(
        model_id: str,
        model_type: str = "diffusion_text2img",
        use_ip_adapter: bool = True,
        ip_adapter_config: dict = None,
    ):
    model_path = download_model(model_id, model_type)
    if model_path is None:
        logger.error("âŒ ëª¨ë¸ ê²½ë¡œê°€ Noneì…ë‹ˆë‹¤. ë¡œë”©ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return None

    model_pipeline = load_model(model_path, model_type)

    # IP-Adapter ì£¼ì… (ì˜µì…˜)
    if use_ip_adapter and not hasattr(model_pipeline, "image_proj_model"):
        try:
            adapter_config = ip_adapter_config or {
                "repo_id": "h94/IP-Adapter",
                "subfolder": "sdxl_models",
                "weight_name": "ip-adapter_sdxl.bin",
                "scale": 0.8,
            }
            model_pipeline.load_ip_adapter(
                adapter_config["repo_id"],
                subfolder=adapter_config["subfolder"],
                weight_name=adapter_config["weight_name"],
            )
            model_pipeline.set_ip_adapter_scale(adapter_config["scale"])
            logger.info("âœ… IP-Adapter ìë™ ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ IP-Adapter ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
            
    return model_pipeline


def get_vton_pipeline(
    pipeline_model: str = "diffusers/stable-diffusion-xl-1.0-inpainting-0.1",
    vae_model: str = "madebyollin/sdxl-vae-fp16-fix",
    controlnet_model: str = "diffusers/controlnet-depth-sdxl-1.0",
    ip_adapter_config: dict = {
        "repo_id": "h94/IP-Adapter",
        "subfolder": "sdxl_models",
        "weight_name": "ip-adapter_sdxl.bin",
        "scale": 0.8
    },
    lora_config: dict = {
        "repo_id": "Norod78/weird-fashion-show-outfits-sdxl-lora",
        "weight_name": "sdxl-WeirdOutfit-Dreambooh.safetensors"
    },
):
    """
    Stable Diffusion ê¸°ë°˜ VTON íŒŒì´í”„ë¼ì¸ì„ í•œ ë²ˆì— ì¤€ë¹„í•©ë‹ˆë‹¤.
    (ë‹¤ìš´ë¡œë“œ + ë¡œë“œ + ì£¼ì… + IP-Adapter + LoRA ì ìš©)

    Args:
        pipeline_model (str): Inpainting íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ID
        vae_model (str): VAE ëª¨ë¸ ID
        controlnet_model (str): ControlNet ëª¨ë¸ ID
        ip_adapter_config (dict): IP-Adapter ì„¤ì • {repo_id, subfolder, weight_name, scale}
        lora_config (dict): LoRA ì„¤ì • {repo_id, weight_name}
        save_dir (str): ëª¨ë¸ ì €ì¥ ê¸°ë³¸ ê²½ë¡œ

    Returns:
        pipeline: GPU ë¡œë“œ ì™„ë£Œëœ ìµœì¢… íŒŒì´í”„ë¼ì¸
    """
    logger.debug("ğŸ› ï¸ vton íŒŒì´í”„ë¼ì¸ êµ¬ì„±ìš”ì†Œ ë‹¤ìš´ë¡œë“œ ë° ë¡œë”© ì‹œì‘")

    # ë‹¤ìš´ë¡œë“œ
    logger.debug("ğŸ› ï¸ íŒŒì´í”„ë¼ì¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    pipeline_path = download_model(pipeline_model, model_type="diffusion_pipeline")
    vae_path = download_model(vae_model, model_type="vae")
    controlnet_path = download_model(controlnet_model, model_type="controlnet")

    if not all([pipeline_path, vae_path, controlnet_path]):
        logger.error("âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: í•˜ë‚˜ ì´ìƒì˜ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ")
        return None

    # ë¡œë“œ
    logger.debug("ğŸ› ï¸ íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹œì‘")
    vae = AutoencoderKL.from_pretrained(vae_path, torch_dtype=torch.float16)
    pipeline = AutoPipelineForInpainting.from_pretrained(
        pipeline_path,
        vae=vae,
        torch_dtype=torch.float16,
        use_safetensors=True
    ).to("cuda")
    controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)

    # ì£¼ì…
    logger.debug("ğŸ› ï¸ êµ¬ì„±ìš”ì†Œ ì£¼ì… ì‹œì‘")
    pipeline.controlnet = controlnet

    # IP-Adapter ì ìš©
    try:
        pipeline.load_ip_adapter(
            ip_adapter_config["repo_id"],
            subfolder=ip_adapter_config.get("subfolder", "sdxl_models"),
            weight_name=ip_adapter_config["weight_name"]
        )
        pipeline.set_ip_adapter_scale(ip_adapter_config.get("scale", 0.8))
        logger.info("âœ… IP-Adapter ì ìš© ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ IP-Adapter ì ìš© ì‹¤íŒ¨: {e}")

    # LoRA ì ìš©
    try:
        pipeline.load_lora_weights(
            lora_config["repo_id"],
            weight_name=lora_config["weight_name"]
        )
        logger.info("âœ… LoRA ì ìš© ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ LoRA ì ìš© ì‹¤íŒ¨: {e}")

    return pipeline